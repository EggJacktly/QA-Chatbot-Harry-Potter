{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93c9eec",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| Version | Date     | Creator          | Change description                               |\n",
    "|---------|----------|------------------|--------------------------------------------------|\n",
    "| v0.05   | 07/09/23 | Jaikishan Khatri | Model comparison and tuning for better outputs   |\n",
    "| v0.04   | 06/09/23 | Jaikishan Khatri | Memory integration for chat functionality        |\n",
    "| v0.03   | 05/09/23 | Jaikishan Khatri | Trial of different embedding models              |\n",
    "| v0.02   | 04/09/23 | Jaikishan Khatri | Generation with diff locat LLM models            |\n",
    "| v0.01   | 03/09/23 | Jaikishan Khatri | Loader, Splitter, Storage, Retreival, Generation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417593b4",
   "metadata": {},
   "source": [
    "# QA Chatbot for parsing Harry Potter books to generate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b615d72",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "According to ðŸ¦œðŸ”—*LangChain*, process for transforming unstructured raw data into a QA chain is as follows:\n",
    "\n",
    "1. **Loading**: We must load our data first. Numerous sources can be used to load unstructured data.\n",
    "2. **Splitting**: Documents are divided into splits of a predetermined size using text splitters. \n",
    "3. **Storage**: The splits will be stored and frequently embedded in storage (such as a vectorstore).\n",
    "4. **Retrieval**: The app fetches splits from storage (for instance, frequently with embeddings similar to the input query).\n",
    "5. **Generation**: An LLM generates a response using a prompt that contains the query and the data that was retrieved. \n",
    "6. **Conversation** (Extension): Adds Memory to the QA chain to hold a multi-turn dialogue.\n",
    "\n",
    "![LLM-QA-flowchart.jpeg](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)\n",
    "\n",
    "Image source: [LangChain](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d119f6",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd246a8c",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "! pip install -qq -U langchain tiktoken pypdf faiss-gpu\n",
    "! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
    "! pip install -qq -U accelerate bitsandbytes xformers einops gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0c709",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c565952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unix_jk/.local/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n",
      "2023-09-07 20:11:26.507345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 20:11:26.705343: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-07 20:11:27.553363: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 20:11:27.553491: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 20:11:27.553498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cabc118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcc679",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Major Hyperparameters which can be used to tune the output of LLMs:  \n",
    "- `temperature` is a hyperparameter that controls the randomness of language model output.  \n",
    "- `top_p` also known as nucleus sampling, is another hyperparameter that controls the randomness of language model output. It sets a threshold probability and selects the top tokens whose cumulative probability exceeds the threshold. The model then randomly samples from this set of tokens to generate output.  \n",
    "- `max_length`\n",
    "- `repetition_penalty`\n",
    "- `chunk_size` is a hyperparameter for embeddings model which changes the size of chunks created by splitting.\n",
    "- `chunk_overlap` is another hyperparameter for embeddings model to have overlap between two chunks, this in turn helps in reducing data loss because of low semantic chunks created by the splitter.\n",
    "- `k` is the number of document chunks to feed the LLM model as context after extracting from the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abb5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # LLMs\n",
    "    model_name = 'vicuna' # wizardlm, bloom, falcon, llama2-7b, Photolens-llama-2-7b, vicuna\n",
    "    temperature = 0 # using temperature 0 or 0.1 because we don't want to go out of context\n",
    "    top_p = 0.95\n",
    "    repetition_penalty = 1\n",
    "    \n",
    "    # splitting\n",
    "    split_chunk_size = 500\n",
    "    split_overlap = 100\n",
    "    \n",
    "    # embeddings\n",
    "    embeddings_model_repo = 'intfloat/multilingual-e5-large' \n",
    "    \n",
    "    ### English major embedding models\n",
    "    # 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "    # 'sentence-transformers/all-MiniLM-L6-v2', \n",
    "    # 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "    # 'sentence-transformers/multi-qa-distilbert-cos-v1'\n",
    "    # BAAI/bge-large-en\n",
    "    \n",
    "    ### Spanish/multilingual embedding models\n",
    "    # intfloat/multilingual-e5-large\n",
    "    \n",
    "    # retriever\n",
    "    retriever_type = 'similarity_search' # 'similarity_search', 'MultiQueryRetriever', 'Max marginal relevance', 'SVMRetriever'\n",
    "    \n",
    "    # create a new vectorstore, False for using pre built vectorstore\n",
    "    new_vectorstore = True\n",
    "    \n",
    "    # number of extracted passages\n",
    "    k = 5\n",
    "    \n",
    "    # quantization config\n",
    "#     quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "    \n",
    "    # paths\n",
    "    pdfs_path = './data/hp-books-en/'\n",
    "    embeddings_path =  './data/vectorstore-en'\n",
    "    Persist_directory = './data/vectorstore-en/' \n",
    "    offload_folder = './offload_folder/'\n",
    "    csv_path = './model_comparison/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e744b7",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "# sentence transformers\n",
    "all-mpnet-base-v2\n",
    "\n",
    "sentence-transformers/all-MiniLM-L12-v2\n",
    "sentence-transformers/all-MiniLM-L6-v2\n",
    "hkunlp/instructor-large\n",
    "\n",
    "\n",
    "sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "sentence-transformers/multi-qa-distilbert-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdea2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name = CFG.model_name):\n",
    "    \n",
    "    \"\"\" Returns the tokenizer and model for the specified model name.\n",
    "    Models are currently selected based on the ability to run on local machine with 32 GB Memory and 8 GB Cuda RAM to run.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : str\n",
    "        Name of the model to be used.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    tokenizer : transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
    "        Tokenizer for the specified model.\n",
    "    model : transformers.modeling_utils.PreTrainedModel\n",
    "        Model for the specified model.\n",
    "    max_len : int\n",
    "        Maximum length of the input sequence for the specified model.\n",
    "    \"\"\"\n",
    "    if model_name == 'vicuna':\n",
    "        model_repo = 'lmsys/vicuna-7b-v1.3'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_length = 4096    \n",
    "        \n",
    "    elif model_name == 'wizardlm':\n",
    "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        max_length = 4096  \n",
    "        \n",
    "    elif model_name == 'wizardlm-13b':\n",
    "        model_repo = 'WizardLM/WizardLM-13B-V1.2'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_length = 4096\n",
    "        \n",
    "    elif model_name == 'Photolens-llama-2-7b':\n",
    "        model_repo = 'Photolens/llama-2-7b-langchain-chat'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_length = 4096\n",
    "        \n",
    "    elif model_name == 'llama2-7b':\n",
    "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_length = 4096\n",
    "\n",
    "    elif model_name == 'bloom':\n",
    "        model_repo = 'bigscience/bloom-7b1'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        \n",
    "        max_length = 4096\n",
    "\n",
    "    elif model_name == 'falcon':\n",
    "        model_repo = 'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2'\n",
    "        \n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model_name = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_length = 4096\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Incorrect Model Name')\n",
    "\n",
    "    return model_tokenizer, model_name, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8a3858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:24:58.623948800Z",
     "start_time": "2023-08-31T11:24:57.949747500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-26T19:52:47.324087Z",
     "iopub.status.busy": "2023-08-26T19:52:47.323715Z",
     "iopub.status.idle": "2023-08-26T19:58:04.634613Z",
     "shell.execute_reply": "2023-08-26T19:58:04.633514Z",
     "shell.execute_reply.started": "2023-08-26T19:52:47.32406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading model:  vicuna \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cb26654dda4a3a9c7bc57a934296bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 5.81 s, sys: 7.74 s, total: 13.5 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer, model, max_len = get_model(model_name= CFG.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d48e0",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Create a pipeline for the model using HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ae9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_length = max_len,\n",
    "    temperature = CFG.temperature,\n",
    "    top_p = CFG.top_p,\n",
    "    repetition_penalty = CFG.repetition_penalty\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1c7e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fb7f36d5de0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b076785",
   "metadata": {},
   "source": [
    "### Step 1: Loading\n",
    "\n",
    "- Load data (here Harry Potter PDF books) and parse data from a directory and convert them into text.  \n",
    "- Using `DirectoryLoader` to load the directory of the PDF documents.  \n",
    "- Benefits of using `PyPDFLoader` is that it creates chunks at character level and also stores page numbers in metadata which can be used to reference the source files.\n",
    "- Loading can be done by multiple ways as mentioned in the [LangChain Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders.html) section.\n",
    "\n",
    "**Note:**\n",
    "The LangChain integration portal currently has [157 Document Loaders](https://integrations.langchain.com/). Each loader produces a LangChain Document as the data output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a337672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T20:21:04.875625100Z",
     "start_time": "2023-09-07T20:21:04.835459700Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_raw_pdf(pdfs_path):\n",
    "    \"\"\" Loads PDF documents from a directory and converts them into text.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdfs_path : str\n",
    "        Path to the directory containing PDF documents.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    documents : list\n",
    "        List of LangChain Documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    loader = DirectoryLoader(\n",
    "        pdfs_path,\n",
    "        loader_cls=PyPDFLoader,\n",
    "        glob=\"./*.pdf\",\n",
    "        show_progress=True,\n",
    "        use_multithreading=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e39fba",
   "metadata": {},
   "source": [
    "### Step 2: Splitter\n",
    "\n",
    "- Split the text up into small, semantically meaningful chunks (often sentences) of predefined sizes. This helps in creating smaller batches of data to be embedded in VectorStore. \n",
    "- These semantically related pieces of text are stored closer to each other for better extraction.\n",
    "- Benefit of using `RecursiveCharacterTextSplitter` is that it splits text by recursively looking at characters. Recursively tries to split by different characters to find one that works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15214f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_chunks(documents, split_chunk_size: int=500, split_overlap: int=0):\n",
    "    \n",
    "    \"\"\" Splits the documents into chunks of predefined size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    documents : list\n",
    "        List of LangChain Documents.\n",
    "    split_chunk_size : int\n",
    "        Size of the chunks to be created from the documents.\n",
    "    split_overlap : int\n",
    "        Overlap between two chunks.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    doc_chunks : list\n",
    "        List of LangChain Documents.   \n",
    "    \"\"\"\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=split_chunk_size,\n",
    "        chunk_overlap=split_overlap,\n",
    "        separators = [\"\\n\\n\", \"\\n\", \"\\t\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    doc_chunks = text_splitter.split_documents(documents)\n",
    "    return doc_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8191322",
   "metadata": {},
   "source": [
    "### Step 3: Store\n",
    "\n",
    "- Embedding models are used to embedded sentences (Performance Sentence Embeddings) and to embedded search queries & paragraphs (Performance Semantic Search).  \n",
    "- The current embedding model was selected from the list of best [pre-trained sentence transformers](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models) which are hosted on HuggingFace.\n",
    "- The benefit of using [FAISS vector store](https://python.langchain.com/docs/integrations/vectorstores/faiss)  is that it can use GPU for constructing embeddings which is faster that many other Vector Stores. It also stores the embeddings in local directory to be used by the models locally.\n",
    "\n",
    "**Note:**\n",
    "The LangChain integration portal currently has [53 VectorStores](https://integrations.langchain.com/vectorstores) and [38 Embedding Models](https://integrations.langchain.com/embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452c14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_new_vectorstore(pdfs_path: str, split_chunk_size: int, split_overlap: int, \n",
    "                            embeddings_model_repo: str, embeddings_path: str = './data/vectorstore-en'):\n",
    "    \"\"\" Creates a new vectorstore and embeddings model. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdfs_path : str\n",
    "        Path to the directory containing PDF documents.\n",
    "    split_chunk_size : int\n",
    "        Size of the chunks to be created from the documents.\n",
    "    split_overlap : int\n",
    "        Overlap between two chunks.\n",
    "    embeddings_model_repo : str\n",
    "        Name of the embeddings model to be used.\n",
    "    embeddings_path : str\n",
    "        Path to the directory where the vectorstore is to be stored.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vectorstore : langchain.vectorstores.faiss.FAISS\n",
    "        Vectorstore containing the embeddings of the documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # load PDF documents\n",
    "    documents = get_raw_pdf(pdfs_path)\n",
    "    \n",
    "    # split them into chunks\n",
    "    doc_chunks = get_document_chunks(documents, split_chunk_size, split_overlap)\n",
    "    \n",
    "    # create embeddings model\n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name = embeddings_model_repo,\n",
    "                                               model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "    # create new_vectorstore\n",
    "    vectorstore = FAISS.from_documents(documents = doc_chunks, \n",
    "                                       embedding = embeddings)\n",
    "\n",
    "    # persist vector database\n",
    "    vectorstore.save_local(embeddings_path)\n",
    "    \n",
    "    return vectorstore, embeddings\n",
    "\n",
    "def _load_prev_vectorstore(embeddings_model_repo, embeddings_path):\n",
    "    \"\"\" Loads a previously created vectorstore and embeddings model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_model_repo : str\n",
    "        Name of the embeddings model to be used.\n",
    "    embeddings_path : str\n",
    "        Path to the directory where the vectorstore is to be stored.\n",
    "                \n",
    "    Returns\n",
    "    -------\n",
    "    vectorstore : langchain.vectorstores.faiss.FAISS\n",
    "        Vectorstore containing the embeddings of the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # download embeddings model\n",
    "    model_embeddings = HuggingFaceInstructEmbeddings(model_name = embeddings_model_repo,\n",
    "                                                     model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "    # load vectorstore and embeddings\n",
    "    vec_store = FAISS.load_local(embeddings_path, model_embeddings)\n",
    "    \n",
    "    return vec_store, model_embeddings\n",
    "\n",
    "\n",
    "def create_vectorstore_embeddings(pdfs_path: str, split_chunk_size, split_overlap, \n",
    "                                  embeddings_model_repo: str, embeddings_path: str = './data/vectorstore-en',\n",
    "                                  new_vectorstore: bool=False):\n",
    "    \n",
    "    \"\"\" Creates a new vectorstore and embeddings model if new_vectorstore is True else loads a previously created vectorstore and embeddings model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pdfs_path : str\n",
    "        Path to the directory containing PDF documents.\n",
    "    split_chunk_size : int\n",
    "        Size of the chunks to be created from the documents.\n",
    "    split_overlap : int\n",
    "        Overlap between two chunks.\n",
    "    embeddings_model_repo : str\n",
    "        Name of the embeddings model to be used.\n",
    "    embeddings_path : str\n",
    "        Path to the directory where the vectorstore is to be stored.\n",
    "    new_vectorstore : bool\n",
    "        If True, creates a new vectorstore and embeddings model else loads a previously created vectorstore and embeddings model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vectorstore : langchain.vectorstores.faiss.FAISS\n",
    "        Vectorstore containing the embeddings of the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not new_vectorstore:\n",
    "        if os.path.isfile('./data/vectorstore-en/index.faiss'):\n",
    "            vec_store, model_embeddings = _load_prev_vectorstore(embeddings_model_repo, embeddings_path)\n",
    "            \n",
    "        else:\n",
    "            vec_store, model_embeddings = _create_new_vectorstore(pdfs_path, split_chunk_size, split_overlap, embeddings_model_repo, embeddings_path)\n",
    "    \n",
    "    else:\n",
    "        vec_store, model_embeddings = _create_new_vectorstore(pdfs_path, split_chunk_size, split_overlap, embeddings_model_repo, embeddings_path)\n",
    "     \n",
    "        \n",
    "    return vec_store, model_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a4f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:34<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "vectorstore, embeddings = create_vectorstore_embeddings(CFG.pdfs_path,\n",
    "                                                        CFG.split_chunk_size,\n",
    "                                                        CFG.split_overlap,\n",
    "                                                        CFG.embeddings_model_repo,\n",
    "                                                        CFG.embeddings_path,\n",
    "                                                        new_vectorstore=CFG.new_vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788ad12",
   "metadata": {},
   "source": [
    "### Step 4. Retrieve\n",
    "- A retriever is an interface that returns documents given an unstructured query. Vector Stores can be taken as retrievers to retrieve relevant documents.\n",
    "\n",
    "- Different types of retrieval methods include Similarity search, MultiQueryRetriever, Max marginal relevance, SVMRetriever  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4442d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5c049",
   "metadata": {},
   "source": [
    "### Custom Prompt\n",
    "\n",
    "- Prompt engineering is the process of structuring text that can be interpreted and understood by a generative AI model. It is a part of tuning methodology for generating better outputs from LLMs. .  \n",
    "- The context is extracted from the Retriever and passed into the `context` variable and the query or user question is passed into `question` variable and passed through `PromptTemplate` to create a custom prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729b558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Answer in the same language the question was asked.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2338c9bd",
   "metadata": {},
   "source": [
    "def get_retriever(retriever_type: str = 'similarity_search', k: int = 4, question: str, vectorstore, all_splits, embeddings):\n",
    "    \n",
    "    # searches for similarity among the retreived documents\n",
    "    if retriever_type == 'similarity_search':\n",
    "        retriever = vectorstore.as_retriever(search_kwargs = {'k': k, 'search_type' : 'similarity'})\n",
    "    \n",
    "    # selects for relevance and diversity among the retrieved documents\n",
    "    elif retriever_type == 'Max marginal relevance':\n",
    "        retriever = vectorstore.as_retriever(search_kwargs = {'k': k, 'search_type' : 'mmr'})\n",
    "        \n",
    "    elif retriever_type == 'SVMRetriever':\n",
    "        svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n",
    "        retriever_docs = svm_retriever.get_relevant_documents(question)\n",
    "        \n",
    "    # generates variants of the input question to improve retrieval using llm\n",
    "#     elif retriever_type == 'MultiQueryRetriever':\n",
    "#         retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(), llm=llm)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Incorrect Retriever Type')\n",
    "        \n",
    "    return retriever_docs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f78eea9f",
   "metadata": {},
   "source": [
    "svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "len(docs_svm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e0faf2",
   "metadata": {},
   "source": [
    "docs_svm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b987d15c",
   "metadata": {},
   "source": [
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "879d9bab",
   "metadata": {},
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d45e86",
   "metadata": {},
   "source": [
    "### Step 5: Generate\n",
    "Retriever chain for QA\n",
    "- The LangChain integration portal currently has 68 LLMs and 13 Chat Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b82be555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import GPT4All\n",
    "\n",
    "# llm_gpt = GPT4All(model='./models/nous-hermes-13b.ggmlv3.q4_0', max_tokens=4096, n_threads = 12)\n",
    "\n",
    "# nous-hermes-13b.ggmlv3.q4_0\n",
    "# GPT4All-13B-snoozy.ggmlv3.q4_0\n",
    "# ggml-gpt4all-j-v1.3-groovy\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c8293",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b83d66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_ans(user_query, model_answer, answer_dict):\n",
    "    \"\"\" Compares the answers from different models and stores them in a dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user_query : str\n",
    "        Query or question asked by the user.    \n",
    "    model_answer : dict\n",
    "        Answer returned by the model.\n",
    "    answer_dict : dict\n",
    "        Dictionary to store the answers from different models.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    ans_dict : dict\n",
    "        Dictionary with answers from different models.\n",
    "    \"\"\"\n",
    "    \n",
    "    if answer_dict is None:\n",
    "        answer_dict = {user_query: model_answer['result']}\n",
    "    else:\n",
    "        answer_dict = {**answer_dict, **{user_query: model_answer['result']}}\n",
    "    return answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d5f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hagrid's favorite animals are the Kneazles and the dragons.\n",
      "CPU times: user 1.49 s, sys: 332 ms, total: 1.82 s\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans_dict = None\n",
    "\n",
    "query = \"Which are Hagrid's favorite animals?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe786097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Harry faces three challenges during the Triwizard Tournament: navigating the swamp, overcoming the dragon, and solving the riddle.\n",
      "CPU times: user 2.76 s, sys: 541 ms, total: 3.3 s\n",
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Which challenges does Harry face during the Triwizard Tournament?\"\n",
    "\n",
    "ans = qa_chain(query)\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49799647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Polyjuice Potion: This potion allows the drinker to transform into anyone they can see in their mind. It's used by Harry in Harry Potter and the Goblet of Fire to impersonate another student during the Triwizard Tournament.\n",
      "2. Felix Felicis: This potion, also known as \"liquid luck,\" is consumed to enhance one's luck and increase their chances of success. It's featured in Harry Potter and the Potions Master, where Harry drinks it to pass his practical exam.\n",
      "3. Unicorn Blood Potion: This potion is used to heal wounds and cure illnesses. It's mentioned in Harry Potter and the Philosopher's Stone when Harry drinks it to recover from the Dursleys' abuse.\n",
      "4. Skele-Gro: This potion is used to revive the dead. It's mentioned in Harry Potter and the Goblet of Fire when Harry uses it to bring back the mischievous ghost, Moaning Myrtle.\n",
      "5. Amortentia: This potion is used to create a scent that represents one's deepest desires. It's featured in Harry Potter and the Half-Blood Prince when Harry uses it to find out what Dumbledore's desires are.\n",
      "CPU times: user 16.5 s, sys: 2.08 s, total: 18.6 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bb860f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " George, Fred, Ron, Ginny, Bill, Charlie, and Percy.\n",
      "CPU times: user 1.38 s, sys: 350 ms, total: 1.73 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Name all seven Weasley children.\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6508b769",
   "metadata": {},
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25854f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Moony, Wormtail, Padfoot, and Prongs are code names for the four Marauders: James Potter, Sirius Black, Peter Pettigrew, and Remus Lupin.\n",
      "CPU times: user 2.82 s, sys: 661 ms, total: 3.48 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Moony, Wormtail, Padfoot, and Prongs are code names for which four characters?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7b41af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 8.00 GiB total capacity; 7.12 GiB already allocated; 0 bytes free; 7.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    290\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    291\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 292\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    293\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    285\u001B[0m             outputs = (\n\u001B[0;32m--> 286\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    137\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m             \u001B[0mdocs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_docs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquestion\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 139\u001B[0;31m         answer = self.combine_documents_chain.run(\n\u001B[0m\u001B[1;32m    140\u001B[0m             \u001B[0minput_documents\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mquestion\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mquestion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_run_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_child\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m         )\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    490\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    491\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 492\u001B[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001B[0m\u001B[1;32m    493\u001B[0m                 \u001B[0m_output_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    494\u001B[0m             ]\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    290\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    291\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 292\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    293\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    285\u001B[0m             outputs = (\n\u001B[0;32m--> 286\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    103\u001B[0m         \u001B[0;31m# Other keys are assumed to be needed for LLM prediction\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m         \u001B[0mother_keys\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mk\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_key\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 105\u001B[0;31m         output, extra_return_dict = self.combine_docs(\n\u001B[0m\u001B[1;32m    106\u001B[0m             \u001B[0mdocs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_run_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_child\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mother_keys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m         )\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py\u001B[0m in \u001B[0;36mcombine_docs\u001B[0;34m(self, docs, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    169\u001B[0m         \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m         \u001B[0;31m# Call predict on the LLM.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mllm_chain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m     async def acombine_docs(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    253\u001B[0m                 \u001B[0mcompletion\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mllm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0madjective\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"funny\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    254\u001B[0m         \"\"\"\n\u001B[0;32m--> 255\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_key\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    256\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    257\u001B[0m     \u001B[0;32masync\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mapredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mCallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    290\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    291\u001B[0m             \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 292\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    293\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_chain_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    294\u001B[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    285\u001B[0m             outputs = (\n\u001B[0;32m--> 286\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0mrun_manager\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCallbackManagerForChainRun\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m     ) -> Dict[str, str]:\n\u001B[0;32m---> 91\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_outputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/llm.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m     99\u001B[0m         \u001B[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprep_prompts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m         return self.llm.generate_prompt(\n\u001B[0m\u001B[1;32m    102\u001B[0m             \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m             \u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36mgenerate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    484\u001B[0m     ) -> LLMResult:\n\u001B[1;32m    485\u001B[0m         \u001B[0mprompt_strings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprompts\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 486\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt_strings\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    487\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    488\u001B[0m     async def agenerate_prompt(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    619\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mcallback_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprompt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallback_managers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprompts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    620\u001B[0m             ]\n\u001B[0;32m--> 621\u001B[0;31m             output = self._generate_helper(\n\u001B[0m\u001B[1;32m    622\u001B[0m                 \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_managers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    623\u001B[0m             )\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36m_generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    521\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mrun_manager\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrun_managers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    522\u001B[0m                 \u001B[0mrun_manager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_llm_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 523\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    524\u001B[0m         \u001B[0mflattened_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    525\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmanager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflattened_output\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_managers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflattened_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36m_generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    509\u001B[0m             output = (\n\u001B[0;32m--> 510\u001B[0;31m                 self._generate(\n\u001B[0m\u001B[1;32m    511\u001B[0m                     \u001B[0mprompts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    512\u001B[0m                     \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/base.py\u001B[0m in \u001B[0;36m_generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    998\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mprompt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprompts\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    999\u001B[0m             text = (\n\u001B[0;32m-> 1000\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrun_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_manager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1001\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mnew_arg_supported\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1002\u001B[0m                 \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    165\u001B[0m         \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m     ) -> str:\n\u001B[0;32m--> 167\u001B[0;31m         \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprompt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    168\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipeline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtask\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"text-generation\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Text generation return includes the starter text.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, text_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m               \u001B[0mids\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mgenerated\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m         \"\"\"\n\u001B[0;32m--> 204\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    205\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    206\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpreprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprompt_text\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprefix\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhandle_long_generation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mgenerate_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m             )\n\u001B[1;32m   1128\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1129\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_single\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1130\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1131\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_multi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\u001B[0m in \u001B[0;36mrun_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1134\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mrun_single\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreprocess_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mforward_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1135\u001B[0m         \u001B[0mmodel_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpreprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1136\u001B[0;31m         \u001B[0mmodel_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mforward_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1137\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpostprocess\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpostprocess_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1138\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m   1033\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0minference_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1034\u001B[0m                     \u001B[0mmodel_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_tensor_on_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1035\u001B[0;31m                     \u001B[0mmodel_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mforward_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1036\u001B[0m                     \u001B[0mmodel_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_tensor_on_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_outputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1037\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\u001B[0m in \u001B[0;36m_forward\u001B[0;34m(self, model_inputs, **generate_kwargs)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m         \u001B[0;31m# BS x SL\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 265\u001B[0;31m         \u001B[0mgenerated_sequence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mgenerate_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    266\u001B[0m         \u001B[0mout_b\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgenerated_sequence\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"pt\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mctx_factory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 115\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001B[0m in \u001B[0;36mgenerate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1594\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mgeneration_mode\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mGenerationMode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGREEDY_SEARCH\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1595\u001B[0m             \u001B[0;31m# 11. run greedy search\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1596\u001B[0;31m             return self.greedy_search(\n\u001B[0m\u001B[1;32m   1597\u001B[0m                 \u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1598\u001B[0m                 \u001B[0mlogits_processor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlogits_processor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001B[0m in \u001B[0;36mgreedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   2442\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2443\u001B[0m             \u001B[0;31m# forward pass to get next token\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2444\u001B[0;31m             outputs = self(\n\u001B[0m\u001B[1;32m   2445\u001B[0m                 \u001B[0;34m**\u001B[0m\u001B[0mmodel_inputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2446\u001B[0m                 \u001B[0mreturn_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1499\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1502\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001B[0m in \u001B[0;36mnew_forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_hf_hook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpost_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    807\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    808\u001B[0m         \u001B[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 809\u001B[0;31m         outputs = self.model(\n\u001B[0m\u001B[1;32m    810\u001B[0m             \u001B[0minput_ids\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    811\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1499\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1502\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001B[0m in \u001B[0;36mnew_forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_hf_hook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpost_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    695\u001B[0m                 )\n\u001B[1;32m    696\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 697\u001B[0;31m                 layer_outputs = decoder_layer(\n\u001B[0m\u001B[1;32m    698\u001B[0m                     \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m                     \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1499\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1502\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001B[0m in \u001B[0;36mnew_forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_hf_hook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpost_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001B[0m\n\u001B[1;32m    408\u001B[0m         \u001B[0mresidual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 410\u001B[0;31m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minput_layernorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    411\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m         \u001B[0;31m# Self Attention\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1499\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1500\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1502\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1503\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001B[0m in \u001B[0;36mnew_forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m                 \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mold_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_hf_hook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpost_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodule\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m     84\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0minput_dtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 86\u001B[0;31m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     87\u001B[0m         \u001B[0mvariance\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeepdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     88\u001B[0m         \u001B[0mhidden_states\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhidden_states\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrsqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvariance\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvariance_epsilon\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 8.00 GiB total capacity; 7.12 GiB already allocated; 0 bytes free; 7.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What position does Harry play on the Gryffindor Quidditch team?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "813ed673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The three different types of balls used in Quidditch are the Quaffle, the Bludger, and the Golden Snitch.\n",
      "CPU times: user 2.47 s, sys: 842 ms, total: 3.32 s\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Name the three different types of balls used in Quidditch.\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f4e0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Crookshanks\n",
      "CPU times: user 919 ms, sys: 230 ms, total: 1.15 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is Hermione's cat's name?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e4f50",
   "metadata": {},
   "source": [
    "#### Out of scope questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "916ca8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gandalf was killed by a group of dark creatures in the story.\n",
      "CPU times: user 1.14 s, sys: 290 ms, total: 1.43 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What did Gandalf do in the story?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bca891f",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "query = \"Was Gandalf in the Harry Potter books?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5876a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ron is afraid of spiders.\n",
      "CPU times: user 1.31 s, sys: 531 ms, total: 1.84 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Which insect is Ron afraid of?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "714ab4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is not specified who killed Dobby in the given context.\n",
      "CPU times: user 1.33 s, sys: 350 ms, total: 1.69 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Who killed Dobby?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "644f01c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are seven players on a Quidditch team.\n",
      "CPU times: user 2.53 s, sys: 751 ms, total: 3.29 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"How many players are on a Quidditch team?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b08763d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are seven Quidditch fouls.\n",
      "CPU times: user 2.69 s, sys: 1.62 s, total: 4.3 s\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"How many possible Quidditch fouls are there?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e52f1c",
   "metadata": {},
   "source": [
    "#### Different language questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5315e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Los padres de Harry Potter son muggles, personas sin habilidades mÃ¡gicas.\n",
      "CPU times: user 1.56 s, sys: 441 ms, total: 2 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"Â¿CuÃ¡l es la profesiÃ³n de los padres de Harry Potter?\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69aa09ac",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "query = \"Dame 5 ejemplos de pociones geniales y explica para quÃ© sirven.\"\n",
    "ans = qa_chain(query)\n",
    "\n",
    "ans_dict = compare_model_ans(query, ans, ans_dict)\n",
    "print(ans['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b95181",
   "metadata": {},
   "source": [
    "## Export CSV Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a5dc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# exporting csv of result answers for model comparison\n",
    "def export_results_to_csv(answer_dict, csv_path, model_name, embeddings_model_repo, temp, top_p, r_penalty, chunk_size, overlap):\n",
    "    \n",
    "    ans_df = pd.DataFrame.from_dict([answer_dict])\n",
    "\n",
    "    embeddings_model_repo = embeddings_model_repo.replace('/', '--')\n",
    "\n",
    "    excel = ans_df.to_csv(csv_path \n",
    "                          + model_name + '_' \n",
    "                          + embeddings_model_repo + '_(' \n",
    "                          + str(temp) + '_'\n",
    "                          + str(top_p) + '_'\n",
    "                          + str(r_penalty) + '_'\n",
    "                          + str(chunk_size) + '_'\n",
    "                          + str(overlap) + ')'\n",
    "                          + '.csv', index=False)\n",
    "    \n",
    "    print('Model results saved at '\n",
    "          + csv_path\n",
    "          + ' with the name '\n",
    "          + model_name + '_'\n",
    "          + embeddings_model_repo + '_('\n",
    "          + str(temp) + '_'\n",
    "          + str(top_p) + '_'\n",
    "          + str(r_penalty) + '_'\n",
    "          + str(chunk_size) + '_'\n",
    "          + str(overlap) + ')'\n",
    "          + '.csv')\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71b18018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results saved at ./model_comparison/ with the name vicuna_intfloat--multilingual-e5-large_(0_0.95_1_500_100).csv\n"
     ]
    }
   ],
   "source": [
    "export_results_to_csv(ans_dict,\n",
    "                      CFG.csv_path,\n",
    "                      CFG.model_name, \n",
    "                      CFG.embeddings_model_repo, \n",
    "                      CFG.temperature, \n",
    "                      CFG.top_p, \n",
    "                      CFG.repetition_penalty,\n",
    "                      CFG.split_chunk_size,\n",
    "                      CFG.split_overlap\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2de6f",
   "metadata": {},
   "source": [
    "### Conversational Chatbot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "123eeafb",
   "metadata": {},
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10581f00",
   "metadata": {},
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Answer in the same language the question was asked.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "064a95ce",
   "metadata": {},
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Answer in the same language the question was asked.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Combine the chat history and follow up question into a standalone question. Keep the question same if there is no chat history.\n",
    "Chat History: {chat_history}\n",
    "Follow up question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate(\n",
    "    template = custom_prompt_template, \n",
    "    input_variables = [\"context\", \"chat_history\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54dda3b7",
   "metadata": {},
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = retriever,\n",
    "    condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36b83d66",
   "metadata": {},
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "conv_qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm = llm_gpt,\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be81e99f",
   "metadata": {},
   "source": [
    "query = \"What position does Harry play on the Gryffindor Quidditch team?\"\n",
    "print(qa(query))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "720be031",
   "metadata": {},
   "source": [
    "query = \"What are the names of the three balls?\"\n",
    "print(qa(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dff4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
