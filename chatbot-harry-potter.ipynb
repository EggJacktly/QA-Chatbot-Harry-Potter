{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93c9eec",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| Version | Date     | Creator          | Change description                               |\n",
    "|---------|----------|------------------|--------------------------------------------------|\n",
    "| v0.02   | 04/09/23 | Jaikishan Khatri | Generation with diff models, model comparison    |\n",
    "| v0.01   | 03/09/23 | Jaikishan Khatri | Loader, Splitter, Storage, Retreival, Generation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417593b4",
   "metadata": {},
   "source": [
    "# QA Chatbot for parsing Harry Potter books to generate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b615d72",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "According to LangChain process for transforming unstructured raw data into a QA chain is as follows:\n",
    "\n",
    "1. <b>Loading</b>: We must load our data first. Numerous sources can be used to load unstructured data. The LangChain integration portal currently has 157 Document Loaders. Each loader produces a LangChain Document as the data output.\n",
    "2. <b>Splitting</b>: Documents are divided into splits of a predetermined size using text splitters. \n",
    "3. <b>Storage</b>: The splits will be stored and frequently embedded in storage (such as a vectorstore). The LangChain integration portal currently has 38 Embedding Models and 53 Vector Stores.\n",
    "4. <b>Retrieval</b>: The app fetches splits from storage (for instance, frequently with embeddings similar to the input query).\n",
    "5. <b>Generation</b>: An LLM generates a response using a prompt that contains the query and the data that was retrieved. The LangChain integration portal currently has 68 LLMs and 13 Chat Models.\n",
    "6. <b>Conversation</b> (Extension): Adds Memory to the QA chain to hold a multi-turn dialogue.\n",
    "\n",
    "![LLM-QA-flowchart.jpeg](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)\n",
    "\n",
    "Image source: [LangChain](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d119f6",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd246a8c",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "! pip install -qq -U langchain tiktoken pypdf faiss-gpu\n",
    "! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
    "! pip install -qq -U accelerate bitsandbytes xformers einops gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0c709",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c565952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unix_jk/.local/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n",
      "2023-09-04 23:17:39.053427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-04 23:17:39.263165: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-04 23:17:40.054560: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/unix_jk/miniconda3/envs/tf/lib/\n",
      "2023-09-04 23:17:40.061170: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/unix_jk/miniconda3/envs/tf/lib/\n",
      "2023-09-04 23:17:40.061180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# retrievers\n",
    "from langchain.retrievers import SVMRetriever\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, XLMRobertaForCausalLM\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcc679",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "For manipulable variables in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5abb5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # LLMs\n",
    "    model_name = 'llama2-7b' # mdeberta-v3, wizardlm, bloom, falcon, llama2-7b, llama2-13b, Photolens-llama-2-7b, xlm-roberta\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    repetition_penalty = 1.15    \n",
    "    \n",
    "    # splitting\n",
    "    split_chunk_size = 1000\n",
    "    split_overlap = 0\n",
    "    \n",
    "    # embeddings\n",
    "    embeddings_model_repo = 'sentence-transformers/multi-qa-mpnet-base-dot-v1' \n",
    "    # 'sentence-transformers/all-MiniLM-L6-v2', \n",
    "    # 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "    # 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "    # 'sentence-transformers/multi-qa-distilbert-cos-v1'\n",
    "    \n",
    "    # retriever\n",
    "    retriever_type = 'similarity_search' # 'similarity_search', 'MultiQueryRetriever', 'Max marginal relevance', 'SVMRetriever'\n",
    "    \n",
    "    # number of extracted passages\n",
    "    k = 5\n",
    "    \n",
    "    # quantization config\n",
    "#     quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "    \n",
    "    # paths\n",
    "    PDFs_path = './Data/HP books/'\n",
    "    Embeddings_path =  './faiss_index_hp/'\n",
    "    Persist_directory = './harry-potter-vectorstore/' \n",
    "    offload_folder = './offload_folder/'\n",
    "    csv_path = './model_comparison/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e744b7",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "IProject-10/xlm-roberta-base-finetuned-squad2\n",
    "timpal0l/mdeberta-v3-base-squad2\n",
    "\n",
    "\n",
    "\n",
    "# sentence transformers\n",
    "all-mpnet-base-v2\n",
    "\n",
    "sentence-transformers/all-MiniLM-L12-v2\n",
    "sentence-transformers/all-MiniLM-L6-v2\n",
    "hkunlp/instructor-large\n",
    "\n",
    "\n",
    "sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "sentence-transformers/multi-qa-distilbert-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdea2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model = CFG.model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the tokenizer and model for the specified model name.\n",
    "    \n",
    "    Args:\n",
    "        model (str): model name\n",
    "        \n",
    "    Returns:\n",
    "        tokenizer (transformers.tokenization_utils_base.PreTrainedTokenizerFast): tokenizer for the specified model\n",
    "        model (transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM): model for the specified model\n",
    "        max_len (int): maximum length of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    print('\\nDownloading model: ', model, '\\n\\n')\n",
    "\n",
    "    if model == 'wizardlm':\n",
    "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "        \n",
    "    # cuda error\n",
    "    elif model == 'xlm-roberta':\n",
    "        model_repo = 'IProject-10/xlm-roberta-base-finetuned-squad2'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model = XLMRobertaForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "    \n",
    "    elif model == 'Photolens-llama-2-7b':\n",
    "        model_repo = 'Photolens/llama-2-7b-langchain-chat'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 4096\n",
    "        \n",
    "    elif model == 'llama2-7b':\n",
    "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "\n",
    "    elif model == 'llama2-13b':\n",
    "        model_repo = 'daryl149/llama-2-13b-chat-hf'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            offload_folder=CFG.offload_folder,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "        )\n",
    "        \n",
    "        max_len = 8192\n",
    "\n",
    "    elif model == 'bloom':\n",
    "        model_repo = 'bigscience/bloom-7b1'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "\n",
    "    elif model == 'falcon':\n",
    "        model_repo = 'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "    # tokenizer error\n",
    "    elif model == 'mdeberta-v3':\n",
    "        model_repo = 'timpal0l/mdeberta-v3-base-squad2'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "\n",
    "    else:\n",
    "        print(\"Not implemented model (tokenizer and backbone)\")\n",
    "\n",
    "    return tokenizer, model, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8a3858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:24:58.623948800Z",
     "start_time": "2023-08-31T11:24:57.949747500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-26T19:52:47.324087Z",
     "iopub.status.busy": "2023-08-26T19:52:47.323715Z",
     "iopub.status.idle": "2023-08-26T19:58:04.634613Z",
     "shell.execute_reply": "2023-08-26T19:58:04.633514Z",
     "shell.execute_reply.started": "2023-08-26T19:52:47.32406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading model:  llama2-7b \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcb0063ccea4a8faad1263f72b07944"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 4.38 s, sys: 4.23 s, total: 8.61 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer, model, max_len = get_model(model = CFG.model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec9d4558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:23:12.597594Z",
     "start_time": "2023-08-31T11:23:12.563595Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-26T19:58:04.638322Z",
     "iopub.status.busy": "2023-08-26T19:58:04.637665Z",
     "iopub.status.idle": "2023-08-26T19:58:04.647608Z",
     "shell.execute_reply": "2023-08-26T19:58:04.646542Z",
     "shell.execute_reply.started": "2023-08-26T19:58:04.638285Z"
    }
   },
   "source": [
    "### check how Accelerate split the model across the available devices (GPUs)\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d48e0",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Create a pipeline for the model using HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ae9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_length = max_len,\n",
    "    temperature = CFG.temperature,\n",
    "    top_p = CFG.top_p,\n",
    "    repetition_penalty = CFG.repetition_penalty\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1c7e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7faf6c471a80>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18ad2336",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "### testing model, not using the harry potter books yet\n",
    "### answer is not necessarily related to harry potter\n",
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "llm(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b076785",
   "metadata": {},
   "source": [
    "### Step 1: Loader \n",
    "using PyPDFLoader\n",
    "\n",
    "Load PDF using `pypdf and chunks at character level.\\\n",
    "Loader also stores page numbers in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a337672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:34<00:00,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 s, sys: 1.38 s, total: 34.7 s\n",
      "Wall time: 34.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    CFG.PDFs_path,\n",
    "    glob=\"./*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c013c165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f1ce349",
   "metadata": {},
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e39fba",
   "metadata": {},
   "source": [
    "### Step 2: Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15214f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = CFG.split_chunk_size,\n",
    "    chunk_overlap  = CFG.split_overlap,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8191322",
   "metadata": {},
   "source": [
    "### Step 3: Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452c14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embeddings_model_repo: str, embeddings_path: str = './faiss_index_hp', new_vectorstore: bool=False):\n",
    "    \n",
    "    if new_vectorstore:\n",
    "        \n",
    "        if embeddings_model_repo.startswith('sentence-transformers'):\n",
    "            embeddings = HuggingFaceInstructEmbeddings(model_name = embeddings_model_repo,\n",
    "                                                       model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "        elif embeddings_model_repo.startswith('GPT4All'):\n",
    "            embeddings = HuggingFaceInstructEmbeddings(model_name = embeddings_model_repo,\n",
    "                                                       model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "        # create embeddings and new_vectorstore\n",
    "        vectorstore = FAISS.from_documents(documents = all_splits, \n",
    "                                           embedding = embeddings)\n",
    "\n",
    "        # persist vector database\n",
    "        vectorstore.save_local(\"faiss_index_hp\")\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # download embeddings model\n",
    "        embeddings = HuggingFaceInstructEmbeddings(model_name = CFG.embeddings_model_repo,\n",
    "                                                   model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "        # load vectorstore embeddings\n",
    "        vectorstore = FAISS.load_local(CFG.Embeddings_path, embeddings)\n",
    "        \n",
    "    return vectorstore, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a4f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "vectorstore, embeddings = create_embeddings(CFG.embeddings_model_repo, CFG.Embeddings_path, new_vectorstore=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f6133fb",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "### download embeddings model\n",
    "if CFG.embeddings_model_repo.startswith('sentence-transformers'):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name = CFG.embeddings_model_repo,\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "    )\n",
    "    \n",
    "elif CFG.embeddings_model_repo.startswith('GPT4All'):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name = CFG.embeddings_model_repo,\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "    )\n",
    "    \n",
    "### create embeddings and DB\n",
    "vectorstore = FAISS.from_documents(documents = all_splits, embedding = embeddings)\n",
    "\n",
    "### persist vector database\n",
    "vectorstore.save_local(\"faiss_index_hp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16c93f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are Hagrid's favourite animals?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be62644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='found\\tHagrid,\\tdon’t\\tyou\\tthink?\\tWhy\\tdidn’t\\tI\\tsee\\tit\\tbefore?”\\n\\t\\t\\t\\t\\t\\t“What\\tare\\tyou\\ttalking\\tabout?”\\tsaid\\tRon,\\tbut\\tHarry,\\tsprinting\\tacross\\tthe\\ngrounds\\ttoward\\tthe\\tforest,\\tdidn’t\\tanswer.\\n\\t\\t\\t\\t\\t\\tHagrid\\twas\\tsitting\\tin\\tan\\tarmchair\\toutside\\this\\thouse;\\this\\ttrousers\\tand\\nsleeves\\twere\\trolled\\tup,\\tand\\the\\twas\\tshelling\\tpeas\\tinto\\ta\\tlarge\\tbowl.\\n\\t\\t\\t\\t\\t\\t“Hullo,”\\the\\tsaid,\\tsmiling.\\t“Finished\\tyer\\texams?\\tGot\\ttime\\tfer\\ta\\tdrink?”\\n\\t\\t\\t\\t\\t\\t“Yes,\\tplease,”\\tsaid\\tRon,\\tbut\\tHarry\\tcut\\thim\\toff.\\n\\t\\t\\t\\t\\t\\t“No,\\twe’re\\tin\\ta\\thurry.\\tHagrid,\\tI’ve\\tgot\\tto\\task\\tyou\\tsomething.\\tYou\\tknow\\nthat\\tnight\\tyou\\twon\\tNorbert?\\tWhat\\tdid\\tthe\\tstranger\\tyou\\twere\\tplaying\\tcards\\twith\\nlook\\tlike?”\\n\\t\\t\\t\\t\\t\\t“Dunno,”\\tsaid\\tHagrid\\tcasually,\\t“he\\twouldn’\\ttake\\this\\tcloak\\toff.”\\n\\t\\t\\t\\t\\t\\tHe\\tsaw\\tthe\\tthree\\tof\\tthem\\tlook\\tstunned\\tand\\traised\\this\\teyebrows.\\n\\t\\t\\t\\t\\t\\t“It’s\\tnot\\tthat\\tunusual,\\tyeh\\tget\\ta\\tlot\\to’\\tfunny\\tfolk\\tin\\tthe\\tHog’s\\tHead\\t—\\nthat’s\\tthe\\tpub\\tdown\\tin\\tthe\\tvillage.\\tMighta\\tbin\\ta\\tdragon\\tdealer,\\tmightn’\\the?\\tI\\nnever\\tsaw\\this\\tface,\\the\\tkept\\this\\thood\\tup.”', metadata={'source': 'Data/HP books/Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'page': 191, 'start_index': 0}),\n",
       " Document(page_content='hesitant voice. \\n“Do you think we should go and ask  Hagrid about it \\nall?”', metadata={'source': 'Data/HP books/Harry Potter - Book 2 - The Chamber of Secrets.pdf', 'page': 277, 'start_index': 995}),\n",
       " Document(page_content='“Madame Maxime, o’ co urse!” said Hagrid. \\n“You two made up, ha ve you?” said Ron. \\n“Dunno what yeh’re talkin’ about,” said Hagrid airily, fetching \\nmore cups from the dresser. Wh en he had made tea and offered \\naround a plate of doughy cookies, he leaned back in his chair and \\nsurveyed Harry closely thro ugh his beetle-black eyes. \\n“You all righ’?” he  said gruffly. \\n“Yeah,” said Harry. “No, yeh’re not,” said Hagrid. “ ’ Course yeh’re not. But yeh will \\nbe.” \\nHarry said nothing. “Knew he was goin’ ter come back ,” said Hagrid, and Harry,', metadata={'source': 'Data/HP books/Harry Potter - Book 4 - The Goblet of Fire.pdf', 'page': 733, 'start_index': 974}),\n",
       " Document(page_content='“Yeah — even if yeh jus’ talk ter him a bit,” said Hagrid hopefully.', metadata={'source': 'Data/HP books/Harry Potter - Book 5 - The Order of the Phoenix.pdf', 'page': 708, 'start_index': 1951})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788ad12",
   "metadata": {},
   "source": [
    "### Step 4. Retrieve\n",
    "similarity_search\\\n",
    "MultiQueryRetriever\\\n",
    "Max marginal relevance\\\n",
    "SVMRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5c049",
   "metadata": {},
   "source": [
    "### Promt Template"
   ]
  },
  {
   "cell_type": "raw",
   "id": "904790e3",
   "metadata": {},
   "source": [
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked. \n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729b558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Answer in the same language the question was asked.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4442d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b196a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7faf64106110>, search_type='similarity', search_kwargs={'k': 5, 'search_type': 'similarity'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2338c9bd",
   "metadata": {},
   "source": [
    "def get_retriever(retriever_type: str = 'similarity_search', k: int = 4, question: str, vectorstore, all_splits, embeddings):\n",
    "    \n",
    "    # searches for similarity among the retreived documents\n",
    "    if retriever_type == 'similarity_search':\n",
    "        retriever = vectorstore.as_retriever(search_kwargs = {'k': k, 'search_type' : 'similarity'})\n",
    "    \n",
    "    # selects for relevance and diversity among the retrieved documents\n",
    "    elif retriever_type == 'Max marginal relevance':\n",
    "        retriever = vectorstore.as_retriever(search_kwargs = {'k': k, 'search_type' : 'mmr'})\n",
    "        \n",
    "    elif retriever_type == 'SVMRetriever':\n",
    "        svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n",
    "        retriever_docs = svm_retriever.get_relevant_documents(question)\n",
    "        \n",
    "    # generates variants of the input question to improve retrieval using llm\n",
    "#     elif retriever_type == 'MultiQueryRetriever':\n",
    "#         retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(), llm=llm)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Incorrect Retriever Type')\n",
    "        \n",
    "    return retriever_docs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f78eea9f",
   "metadata": {},
   "source": [
    "svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "len(docs_svm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e0faf2",
   "metadata": {},
   "source": [
    "docs_svm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b987d15c",
   "metadata": {},
   "source": [
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "879d9bab",
   "metadata": {},
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845f4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a1c085a9",
   "metadata": {},
   "source": [
    "pip install gpt4all --upgrade"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ac20021",
   "metadata": {},
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm_gpt = GPT4All(model='./models/ggml-gpt4all-j-v1.3-groovy.bin', n_threads = 8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "010a867d",
   "metadata": {},
   "source": [
    "pip install gpt4all==1.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d45e86",
   "metadata": {},
   "source": [
    "### Step 5: Generate\n",
    "Retriever chain for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82be555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.llms import GPT4All\n",
    "\n",
    "# llm_gpt = GPT4All(model='./models/nous-hermes-13b.ggmlv3.q4_0', max_tokens=4096, n_threads = 12)\n",
    "\n",
    "# nous-hermes-13b.ggmlv3.q4_0\n",
    "# GPT4All-13B-snoozy.ggmlv3.q4_0\n",
    "# ggml-gpt4all-j-v1.3-groovy\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm = llm_gpt,\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1586f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a687ae2b",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "### testing model, not using the harry potter books yet\n",
    "### answer is not necessarily related to harry potter\n",
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "llm(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64be4f3",
   "metadata": {},
   "source": [
    "### Post process outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7413935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f591f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    \n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    llm_response = qa_chain(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c8293",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which are Hagrid's favorite animals?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# create a new dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {query: ans}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe786097",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which challenges does Harry face during the Triwizard Tournament?\"\n",
    "ans = qa_chain(query)['result']\n",
    "# print(llm_ans(query))\n",
    "\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49799647",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "ans = qa_chain(query)['result']\n",
    "print(ans)\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ca8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did Gandalf do in the story?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5315e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"¿Cuál es la profesión de los padres de Harry Potter?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Dame 5 ejemplos de pociones geniales y explica para qué sirven.\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was Gandalf in the Harry Potter books?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb860f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Name all seven Weasley children.\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd5b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"Moony, Wormtail, Padfoot, and Prongs are code names for which four characters?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b41af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What position does Harry play on the Gryffindor Quidditch team?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ed673",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Name the three different types of balls used in Quidditch.\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "ans_dict = {**ans_dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting csv of result answers for model comparison\n",
    "\n",
    "import pandas as pd\n",
    "ans_df = pd.DataFrame.from_dict([ans_dict])\n",
    "\n",
    "model_repo_name = CFG.embeddings_model_repo\n",
    "\n",
    "try:\n",
    "    model_repo_name = CFG.embeddings_model_repo.split('/')[1]\n",
    "except:\n",
    "    model_repo_name = CFG.embeddings_model_repo\n",
    "    \n",
    "ans_df.to_csv(CFG.csv_path + CFG.model_name + '_' + model_repo_name +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b18018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c9229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d2de6f",
   "metadata": {},
   "source": [
    "### Conversational Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf7a2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a641c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Answer in the same language the question was asked.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Combine the chat history and follow up question into a standalone question. Keep the question same if there is no chat history.\n",
    "Chat History: {chat_history}\n",
    "Follow up question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate(\n",
    "    template = custom_prompt_template, \n",
    "    input_variables = [\"context\", \"chat_history\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7519b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nchain_type_kwargs\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_380/2027248263.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m qa = ConversationalRetrievalChain.from_llm(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36mfrom_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         )\n\u001b[0;32m--> 356\u001b[0;31m         return cls(\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nchain_type_kwargs\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = retriever,\n",
    "    condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "conv_qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm = llm_gpt,\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f579226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What position does Harry play on the Gryffindor Quidditch team?', 'chat_history': [HumanMessage(content='What position does Harry play on the Gryffindor Quidditch team?', additional_kwargs={}, example=False), AIMessage(content=' Harry plays the position of Seeker on the Gryffindor House team.', additional_kwargs={}, example=False)], 'answer': ' Harry plays the position of Seeker on the Gryffindor House team.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What position does Harry play on the Gryffindor Quidditch team?\"\n",
    "print(qa(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78c26de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'context'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_380/828234647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What are the names of the three balls?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             outputs = (\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchat_history_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             new_question = self.question_generator.run(\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_history_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    493\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             ]\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0;31m`\u001b[0m\u001b[0mChain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         callback_manager = CallbackManager.configure(\n\u001b[1;32m    270\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mprep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mexternal_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_memory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexternal_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mmissing_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing some input keys: {missing_keys}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'context'}"
     ]
    }
   ],
   "source": [
    "query = \"What are the names of the three balls?\"\n",
    "print(qa(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dff4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
