{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93c9eec",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| Version | Date     | Creator          | Change description                               |\n",
    "|---------|----------|------------------|--------------------------------------------------|\n",
    "| v0.02   | 04/09/23 | Jaikishan Khatri | Generation with diff models, model comparison    |\n",
    "| v0.01   | 03/09/23 | Jaikishan Khatri | Loader, Splitter, Storage, Retreival, Generation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417593b4",
   "metadata": {},
   "source": [
    "# QA Chatbot for parsing Harry Potter books to generate answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b615d72",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "According to LangChain process for transforming unstructured raw data into a QA chain is as follows:\n",
    "\n",
    "1. <b>Loading</b>: We must load our data first. Numerous sources can be used to load unstructured data. The LangChain integration portal currently has 157 Document Loaders. Each loader produces a LangChain Document as the data output.\n",
    "2. <b>Splitting</b>: Documents are divided into splits of a predetermined size using text splitters. \n",
    "3. <b>Storage</b>: The splits will be stored and frequently embedded in storage (such as a vectorstore). The LangChain integration portal currently has 38 Embedding Models and 53 Vector Stores.\n",
    "4. <b>Retrieval</b>: The app fetches splits from storage (for instance, frequently with embeddings similar to the input query).\n",
    "5. <b>Generation</b>: An LLM generates a response using a prompt that contains the query and the data that was retrieved. The LangChain integration portal currently has 68 LLMs and 13 Chat Models.\n",
    "6. <b>Conversation</b> (Extension): Adds Memory to the QA chain to hold a multi-turn dialogue.\n",
    "\n",
    "![LLM-QA-flowchart.jpeg](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)\n",
    "\n",
    "Image source: [LangChain](https://python.langchain.com/assets/images/qa_flow-9fbd91de9282eb806bda1c6db501ecec.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d119f6",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd246a8c",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "! pip install -qq -U langchain tiktoken pypdf faiss-gpu\n",
    "! pip install -qq -U transformers InstructorEmbedding sentence_transformers\n",
    "! pip install -qq -U accelerate bitsandbytes xformers einops gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0c709",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c565952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unix_jk/.local/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n",
      "2023-09-04 16:37:12.315833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-04 16:37:12.535257: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-04 16:37:13.278153: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/unix_jk/miniconda3/envs/tf/lib/\n",
      "2023-09-04 16:37:13.285399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/unix_jk/miniconda3/envs/tf/lib/\n",
      "2023-09-04 16:37:13.285409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# loaders\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "# text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# retreivers\n",
    "from langchain.retrievers import SVMRetriever\n",
    "\n",
    "# prompts\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# vector stores\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, XLMRobertaForCausalLM\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcc679",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "For manipulable variables in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5abb5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # LLMs\n",
    "    model_name = 'llama2-7b' # mdeberta-v3, wizardlm, bloom, falcon, llama2-7b, llama2-13b, Photolens-llama-2-7b, xlm-roberta\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    repetition_penalty = 1.15    \n",
    "    \n",
    "    # splitting\n",
    "    split_chunk_size = 1000\n",
    "    split_overlap = 0\n",
    "    \n",
    "    # embeddings\n",
    "    embeddings_model_repo = 'sentence-transformers/multi-qa-mpnet-base-dot-v1' # 'sentence-transformers/all-MiniLM-L6-v2', \n",
    "    # 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1', GPT4AllEmbeddings(), multi-qa-mpnet-base-dot-v1\n",
    "    \n",
    "    retriever_type = 'similarity_search' # 'similarity_search', 'MultiQueryRetriever', 'Max marginal relevance', 'SVMRetriever'\n",
    "    \n",
    "    # similar passages\n",
    "    k = 5\n",
    "    \n",
    "    # quantization config\n",
    "#     quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "    \n",
    "    # paths\n",
    "    PDFs_path = './Data/HP books/'\n",
    "    Embeddings_path =  './faiss_index_hp/'\n",
    "    Persist_directory = './harry-potter-vectorstore/' \n",
    "    offload_folder = './offload_folder/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e744b7",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "IProject-10/xlm-roberta-base-finetuned-squad2\n",
    "timpal0l/mdeberta-v3-base-squad2\n",
    "\n",
    "\n",
    "\n",
    "# sentence transformers\n",
    "all-mpnet-base-v2\n",
    "\n",
    "sentence-transformers/all-MiniLM-L12-v2\n",
    "sentence-transformers/all-MiniLM-L6-v2\n",
    "hkunlp/instructor-large\n",
    "\n",
    "\n",
    "sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
    "sentence-transformers/multi-qa-distilbert-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c448a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model = CFG.model_name):\n",
    "\n",
    "    print('\\nDownloading model: ', model, '\\n\\n')\n",
    "\n",
    "    if model == 'wizardlm':\n",
    "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "        \n",
    "    # cuda error\n",
    "    elif model == 'xlm-roberta':\n",
    "        model_repo = 'IProject-10/xlm-roberta-base-finetuned-squad2'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model = XLMRobertaForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "    \n",
    "    elif model == 'Photolens-llama-2-7b':\n",
    "        model_repo = 'Photolens/llama-2-7b-langchain-chat'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 4096\n",
    "        \n",
    "    elif model == 'llama2-7b':\n",
    "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "\n",
    "    elif model == 'llama2-13b':\n",
    "        model_repo = 'daryl149/llama-2-13b-chat-hf'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            offload_folder=CFG.offload_folder,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "        )\n",
    "        \n",
    "        max_len = 8192\n",
    "\n",
    "    elif model == 'bloom':\n",
    "        model_repo = 'bigscience/bloom-7b1'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "\n",
    "    elif model == 'falcon':\n",
    "        model_repo = 'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 1024\n",
    "    # tokenizer error\n",
    "    elif model == 'mdeberta-v3':\n",
    "        model_repo = 'timpal0l/mdeberta-v3-base-squad2'\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_repo,\n",
    "            load_in_4bit=True,\n",
    "            device_map='auto',\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        max_len = 2048\n",
    "\n",
    "    else:\n",
    "        print(\"Not implemented model (tokenizer and backbone)\")\n",
    "\n",
    "    return tokenizer, model, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2579e8d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:24:58.623948800Z",
     "start_time": "2023-08-31T11:24:57.949747500Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-26T19:52:47.324087Z",
     "iopub.status.busy": "2023-08-26T19:52:47.323715Z",
     "iopub.status.idle": "2023-08-26T19:58:04.634613Z",
     "shell.execute_reply": "2023-08-26T19:58:04.633514Z",
     "shell.execute_reply.started": "2023-08-26T19:52:47.32406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading model:  llama2-7b \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3749f6462749b488345f5fc9f28a66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 5.63 s, sys: 7.38 s, total: 13 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer, model, max_len = get_model(model = CFG.model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c95e7d47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T11:23:12.597594Z",
     "start_time": "2023-08-31T11:23:12.563595Z"
    },
    "execution": {
     "iopub.execute_input": "2023-08-26T19:58:04.638322Z",
     "iopub.status.busy": "2023-08-26T19:58:04.637665Z",
     "iopub.status.idle": "2023-08-26T19:58:04.647608Z",
     "shell.execute_reply": "2023-08-26T19:58:04.646542Z",
     "shell.execute_reply.started": "2023-08-26T19:58:04.638285Z"
    }
   },
   "source": [
    "### check how Accelerate split the model across the available devices (GPUs)\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d48e0",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be979a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task = \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    max_length = max_len,\n",
    "    temperature = CFG.temperature,\n",
    "    top_p = CFG.top_p,\n",
    "    repetition_penalty = CFG.repetition_penalty\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e2190d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f2c1855c6d0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "482cd1cf",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "### testing model, not using the harry potter books yet\n",
    "### answer is not necessarily related to harry potter\n",
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "llm(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b076785",
   "metadata": {},
   "source": [
    "### Step 1: Loader \n",
    "using PyPDFLoader\n",
    "\n",
    "Load PDF using `pypdf and chunks at character level.\\\n",
    "Loader also stores page numbers in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a337672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:35<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.3 s, sys: 1.43 s, total: 35.7 s\n",
      "Wall time: 35.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    CFG.PDFs_path,\n",
    "    glob=\"./*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c013c165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "460e2f7a",
   "metadata": {},
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e39fba",
   "metadata": {},
   "source": [
    "### Step 2: Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15214f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = CFG.split_chunk_size,\n",
    "    chunk_overlap  = CFG.split_overlap,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8191322",
   "metadata": {},
   "source": [
    "### Step 3: Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452c14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embeddings_model_repo: str, Embeddings_path: str = './faiss_index_hp', new_vectorstore: bool=False):\n",
    "    \n",
    "    if new_vectorstore == True:\n",
    "        \n",
    "        if embeddings_model_repo.startswith('sentence-transformers'):\n",
    "            embeddings = HuggingFaceInstructEmbeddings(model_name = embeddings_model_repo,\n",
    "                                                       model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "        elif embeddings_model_repo.startswith('GPT4All'):\n",
    "            embeddings = HuggingFaceInstructEmbeddings(model_name = embeddings_model_repo,\n",
    "                                                       model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "        ### create embeddings and new_vectorstore\n",
    "        vectorstore = FAISS.from_documents(documents = all_splits, \n",
    "                                           embedding = embeddings)\n",
    "\n",
    "        ### persist vector database\n",
    "        vectorstore.save_local(\"faiss_index_hp\")\n",
    "        \n",
    "    else:\n",
    "\n",
    "        ### download embeddings model\n",
    "        embeddings = HuggingFaceInstructEmbeddings(model_name = CFG.embeddings_model_repo,\n",
    "                                                   model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "        ### load vectorstore embeddings\n",
    "        vectorstore = FAISS.load_local(CFG.Embeddings_path, embeddings)\n",
    "        \n",
    "    return vectorstore, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a4f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a62fc544a149e084d5d061cd29a493"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef21b3c378454c00b02684e24e07ecd5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c1003700d545a3826deb94a87f8b55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80dca584671d4ed4b52c348e885d6d5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c00c3ee2c4cb78fe9d59928d944f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065eec4c55054d279942771246c7a0a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275089a6ae704d29abd31ae8ae409cce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d93d52606cf4244911ee20fe1b805b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59383d842dc741a7b3a54e462e4a5472"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef8bd4649264d1faa4bc95a4ea8a9b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c420c0b00e834bcea8c79ca76dfaa934"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b71309529c4004a9abe86c30c802c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ed6b80321946c6b7d996651ad9cb43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffe60221c1f4847a8b4c2aa66b55223"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "vectorstore, embeddings = create_embeddings(CFG.embeddings_model_repo, CFG.Embeddings_path, new_vectorstore=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f6133fb",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "### download embeddings model\n",
    "if CFG.embeddings_model_repo.startswith('sentence-transformers'):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name = CFG.embeddings_model_repo,\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "    )\n",
    "    \n",
    "elif CFG.embeddings_model_repo.startswith('GPT4All'):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name = CFG.embeddings_model_repo,\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "    )\n",
    "    \n",
    "### create embeddings and DB\n",
    "vectorstore = FAISS.from_documents(documents = all_splits, embedding = embeddings)\n",
    "\n",
    "### persist vector database\n",
    "vectorstore.save_local(\"faiss_index_hp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d16c93f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are Hagrid's favourite animals?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc9b6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='found\\tHagrid,\\tdon’t\\tyou\\tthink?\\tWhy\\tdidn’t\\tI\\tsee\\tit\\tbefore?”\\n\\t\\t\\t\\t\\t\\t“What\\tare\\tyou\\ttalking\\tabout?”\\tsaid\\tRon,\\tbut\\tHarry,\\tsprinting\\tacross\\tthe\\ngrounds\\ttoward\\tthe\\tforest,\\tdidn’t\\tanswer.\\n\\t\\t\\t\\t\\t\\tHagrid\\twas\\tsitting\\tin\\tan\\tarmchair\\toutside\\this\\thouse;\\this\\ttrousers\\tand\\nsleeves\\twere\\trolled\\tup,\\tand\\the\\twas\\tshelling\\tpeas\\tinto\\ta\\tlarge\\tbowl.\\n\\t\\t\\t\\t\\t\\t“Hullo,”\\the\\tsaid,\\tsmiling.\\t“Finished\\tyer\\texams?\\tGot\\ttime\\tfer\\ta\\tdrink?”\\n\\t\\t\\t\\t\\t\\t“Yes,\\tplease,”\\tsaid\\tRon,\\tbut\\tHarry\\tcut\\thim\\toff.\\n\\t\\t\\t\\t\\t\\t“No,\\twe’re\\tin\\ta\\thurry.\\tHagrid,\\tI’ve\\tgot\\tto\\task\\tyou\\tsomething.\\tYou\\tknow\\nthat\\tnight\\tyou\\twon\\tNorbert?\\tWhat\\tdid\\tthe\\tstranger\\tyou\\twere\\tplaying\\tcards\\twith\\nlook\\tlike?”\\n\\t\\t\\t\\t\\t\\t“Dunno,”\\tsaid\\tHagrid\\tcasually,\\t“he\\twouldn’\\ttake\\this\\tcloak\\toff.”\\n\\t\\t\\t\\t\\t\\tHe\\tsaw\\tthe\\tthree\\tof\\tthem\\tlook\\tstunned\\tand\\traised\\this\\teyebrows.\\n\\t\\t\\t\\t\\t\\t“It’s\\tnot\\tthat\\tunusual,\\tyeh\\tget\\ta\\tlot\\to’\\tfunny\\tfolk\\tin\\tthe\\tHog’s\\tHead\\t—\\nthat’s\\tthe\\tpub\\tdown\\tin\\tthe\\tvillage.\\tMighta\\tbin\\ta\\tdragon\\tdealer,\\tmightn’\\the?\\tI\\nnever\\tsaw\\this\\tface,\\the\\tkept\\this\\thood\\tup.”', metadata={'source': 'Data/HP books/Harry Potter - Book 1 - The Sorcerers Stone.pdf', 'page': 191, 'start_index': 0}),\n",
       " Document(page_content='hesitant voice. \\n“Do you think we should go and ask  Hagrid about it \\nall?”', metadata={'source': 'Data/HP books/Harry Potter - Book 2 - The Chamber of Secrets.pdf', 'page': 277, 'start_index': 995}),\n",
       " Document(page_content='“Madame Maxime, o’ co urse!” said Hagrid. \\n“You two made up, ha ve you?” said Ron. \\n“Dunno what yeh’re talkin’ about,” said Hagrid airily, fetching \\nmore cups from the dresser. Wh en he had made tea and offered \\naround a plate of doughy cookies, he leaned back in his chair and \\nsurveyed Harry closely thro ugh his beetle-black eyes. \\n“You all righ’?” he  said gruffly. \\n“Yeah,” said Harry. “No, yeh’re not,” said Hagrid. “ ’ Course yeh’re not. But yeh will \\nbe.” \\nHarry said nothing. “Knew he was goin’ ter come back ,” said Hagrid, and Harry,', metadata={'source': 'Data/HP books/Harry Potter - Book 4 - The Goblet of Fire.pdf', 'page': 733, 'start_index': 974}),\n",
       " Document(page_content='“Yeah — even if yeh jus’ talk ter him a bit,” said Hagrid hopefully.', metadata={'source': 'Data/HP books/Harry Potter - Book 5 - The Order of the Phoenix.pdf', 'page': 708, 'start_index': 1951})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788ad12",
   "metadata": {},
   "source": [
    "### Step 4. Retrieve\n",
    "similarity_search\\\n",
    "MultiQueryRetriever\\\n",
    "Max marginal relevance\\\n",
    "SVMRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5c049",
   "metadata": {},
   "source": [
    "### Promt Template"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3a67ba4",
   "metadata": {},
   "source": [
    "prompt_template = \"\"\"\n",
    "Don't try to make up an answer, if you don't know just say that you don't know.\n",
    "Answer in the same language the question was asked. \n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4f5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Answer in the same language the question was asked.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables = [\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4442d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b196a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f2c0045be80>, search_type='similarity', search_kwargs={'k': 5, 'search_type': 'similarity'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4eb31b0d",
   "metadata": {},
   "source": [
    "def get_retriever(retriever_type: str = 'similarity_search', k: int = 4, question: str, vectorstore, all_splits, embeddings):\n",
    "    \n",
    "    # searches for similarity among the retreived documents\n",
    "    if retriever_type == 'similarity_search':\n",
    "        retriever = vectorstore.as_retriever(search_kwargs = {'k': k, 'search_type' : 'similarity'})\n",
    "    \n",
    "    # selects for relevance and diversity among the retrieved documents\n",
    "    elif retriever_type == 'Max marginal relevance':\n",
    "        retriever = vectorstore.as_retriever(search_kwargs = {'k': k, 'search_type' : 'mmr'})\n",
    "        \n",
    "    elif retriever_type == 'SVMRetriever':\n",
    "        svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n",
    "        retriever_docs = svm_retriever.get_relevant_documents(question)\n",
    "        \n",
    "    # generates variants of the input question to improve retrieval using llm\n",
    "#     elif retriever_type == 'MultiQueryRetriever':\n",
    "#         retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(), llm=llm)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Incorrect Retriever Type')\n",
    "        \n",
    "    return retriever_docs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "592bebdf",
   "metadata": {},
   "source": [
    "svm_retriever = SVMRetriever.from_documents(all_splits, embeddings)\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "len(docs_svm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43ab4ac6",
   "metadata": {},
   "source": [
    "docs_svm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "655dbb57",
   "metadata": {},
   "source": [
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e6e5b7e",
   "metadata": {},
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87889d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "cafb6136",
   "metadata": {},
   "source": [
    "pip install gpt4all --upgrade"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69e45a10",
   "metadata": {},
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm_gpt = GPT4All(model='./models/ggml-gpt4all-j-v1.3-groovy.bin', n_threads = 8)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95d7f18a",
   "metadata": {},
   "source": [
    "pip install gpt4all==1.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d45e86",
   "metadata": {},
   "source": [
    "### Step 5: Generate\n",
    "Retriever chain for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b82be555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.llms import GPT4All\n",
    "\n",
    "# llm_gpt = GPT4All(model='./models/nous-hermes-13b.ggmlv3.q4_0', max_tokens=4096, n_threads = 12)\n",
    "\n",
    "# nous-hermes-13b.ggmlv3.q4_0\n",
    "# GPT4All-13B-snoozy.ggmlv3.q4_0\n",
    "# ggml-gpt4all-j-v1.3-groovy\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm = llm_gpt,\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
    "    retriever = retriever, \n",
    "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
    "    return_source_documents = True,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41777d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "13f64391",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "### testing model, not using the harry potter books yet\n",
    "### answer is not necessarily related to harry potter\n",
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "llm(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b239b",
   "metadata": {},
   "source": [
    "### Post process outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621ee760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textwrap\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57baa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=700):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
    "    \n",
    "    sources_used = ' \\n'.join(\n",
    "        [\n",
    "            source.metadata['source'].split('/')[-1][:-4] + ' - page: ' + str(source.metadata['page'])\n",
    "            for source in llm_response['source_documents']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3616f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_ans(query):\n",
    "    start = time.time()\n",
    "    llm_response = qa_chain(query)\n",
    "    ans = process_llm_response(llm_response)\n",
    "    end = time.time()\n",
    "\n",
    "    time_elapsed = int(round(end - start, 0))\n",
    "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
    "    return ans + time_elapsed_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84ee47",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bd46599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Which are Hagrid's favorite animals?\": \" Hagrid's favorite animal is Norbert, the baby dragon.\"}\n"
     ]
    }
   ],
   "source": [
    "query = \"Which are Hagrid's favorite animals?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# create a new dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {query: ans}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "647dd763",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which challenges does Harry face during the Triwizard Tournament?\"\n",
    "ans = qa_chain(query)['result']\n",
    "# print(llm_ans(query))\n",
    "\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6394cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are five examples of cool potions and their effects:\n",
      "1. \"Sleeping Beauty's Kiss\": This potion causes the drinker to fall into a deep sleep, which can be useful for escaping danger or recovering from exhaustion. However, it also has the side effect of making the drinker extremely vulnerable while unconscious, so it should only be used in emergency situations.\n",
      "2. \"Invisibility Potion\": As its name suggests, this potion makes the drinker invisible, allowing them to move undetected through a crowd or avoid dangerous situations. However, it does not grant the ability to walk through walls or other physical barriers, so use it wisely!\n",
      "3. \"Polyjuice Potion\": This potion allows the drinker to transform into someone else, complete with their appearance, voice, and personality traits. While it can be useful for undercover missions or disguising oneself as a non-threatening target, it also carries risks such as losing one's own identity and being unable to switch back to one's true form.\n",
      "4. \"Leviosaction Potion\": This potion grants the ability to levitate objects using magic, which can be very helpful for moving heavy items or deflecting attacks. However, it also requires a great deal of concentration and control to avoid accidentally lifting something important or hurting someone with an errant Levitation Charm.\n",
      "5. \"Protego Potion\": This potion creates a protective shield around the drinker, absorbing spells cast against them and reducing damage taken from magical attacks. While it provides valuable protection, it does not guarantee invincibility and should be used strategically rather than relying solely on its effects.\n"
     ]
    }
   ],
   "source": [
    "query = \"Give me 5 examples of cool potions and explain what they do\"\n",
    "ans = qa_chain(query)['result']\n",
    "print(ans)\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "979da6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the story, Dumbledore walked towards where the flayed child lay whimpering and led Harry to two seats placed underneath that high, sparkling ceiling.\n"
     ]
    }
   ],
   "source": [
    "query = \"What did Gandalf do in the story?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac6b4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El padre de Harry, James Potter, era un mago y una estrella del quidditch. La madre de Harry, Lily Evans, también era una bruja.\n"
     ]
    }
   ],
   "source": [
    "query = \"¿Cuál es la profesión de los padres de Harry Potter?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9bf42ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dame 5 ejemplos de pociones geniales y explica para qué sirven.\n",
      "Ejemplo 1: \"Sectumsemio\" - This potion makes the drinker see their worst fears come true. It is used to help people overcome phobias or to test one's bravery.\n",
      "Ejemplo 2: \"Alohomora\" - This potion unlocks any locked door or cabinet. It is useful for students who need to get into restricted areas for research purposes.\n",
      "Ejemplo 3: \"Veritaserum\" - This potion forces the truth out of anyone who takes it. It is often used by the Ministry of Magic to extract information from unwilling witnesses.\n",
      "Ejemplo 4: \"Muffliato\" - This potion makes the listener unable to hear certain words or phrases. It is used to protect sensitive information from being overheard.\n",
      "Ejemplo 5: \"Expecto Patronum\" - This potion summons a silvery cloud that takes the form of a familiar animal. The patronus can be used to defend against dark magic attacks.\n"
     ]
    }
   ],
   "source": [
    "query = \"Dame 5 ejemplos de pociones geniales y explica para qué sirven.\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67563e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, Gandalf is a character from J.R.R Tolkien's Middle-earth stories, not the Harry Potter series.\n"
     ]
    }
   ],
   "source": [
    "query = \"Was Gandalf in the Harry Potter books?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ab4de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fred, George, Ron, Charlus (Charlie), Percy, Bill, and Ginny Weasley.\n"
     ]
    }
   ],
   "source": [
    "query = \"Name all seven Weasley children.\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de1f04c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ron Weasley, Hermione Granger, Albus Dumbledore, and Sirius Black.\n"
     ]
    }
   ],
   "source": [
    "query = \"Moony, Wormtail, Padfoot, and Prongs are code names for which four characters?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "314f5c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Harry plays the position of Seeker on the Gryffindor House team.\n"
     ]
    }
   ],
   "source": [
    "query = \"What position does Harry play on the Gryffindor Quidditch team?\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be1bb5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The three different types of balls used in Quidditch are the Quaffle, the Bludger, and the Golden Snitch.\n"
     ]
    }
   ],
   "source": [
    "query = \"Name the three different types of balls used in Quidditch.\"\n",
    "# print(llm_ans(query))\n",
    "\n",
    "# add to the dictionary\n",
    "ans = qa_chain(query)['result']\n",
    "dict = {**dict, **{query: ans}}\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ae2be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "081484e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving csv of results\n",
    "\n",
    "import pandas as pd\n",
    "ans_df = pd.DataFrame.from_dict([dict])\n",
    "\n",
    "csv_path = './results/'\n",
    "\n",
    "try:\n",
    "    model_repo_name = CFG.embeddings_model_repo.split('/')[1]\n",
    "except:\n",
    "    model_repo_name = CFG.embeddings_model_repo\n",
    "    \n",
    "ans_df.to_csv(csv_path + CFG.model_name + '_' + model_repo_name +'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575e639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
